{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOdt5E3wBRR8Tahfl7YTT50",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Datalincy/EDA/blob/main/Transformers_Model_with_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zNPDvbSmo1pJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing libraries"
      ],
      "metadata": {
        "id": "F9mkD0Edo2O_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "sj7CB-HK-ZI1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IXowOWSao84J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the dataset"
      ],
      "metadata": {
        "id": "3AKENQR8o9Q_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the English-French translation dataset\n",
        "data_path = keras.utils.get_file(\n",
        "    \"fra-eng.zip\", origin=\"http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\", extract=True,)\n",
        "data_path = data_path.replace(\".zip\", \"\")"
      ],
      "metadata": {
        "id": "L8RyDjIP-_vw"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "keras.utils.get_file: This function is used to download a file from a URL and cache it locally. It takes in several arguments:\n",
        "\n",
        "\"fra-eng.zip\": This is the name you want to assign to the downloaded file.\n",
        "\n",
        "origin: The URL from which to download the dataset. In this case, it's pointing to a dataset of English-French translations hosted on Google's cloud storage.\n",
        "\n",
        "extract=True: This argument specifies that the zip file should be extracted automatically after being downloaded."
      ],
      "metadata": {
        "id": "KFBBOFi1qJ3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(data_path + \"/fra.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    english, french = line.split(\"\\t\")\n",
        "    french = \"[start]\" + french + \"[end]\"\n",
        "    text_pairs.append((english, french))\n",
        "english_tokenizer = keras.preprocessing.text.Tokenizer(filters=\"\")\n",
        "french_tokenizer = keras.preprocessing.text.Tokenizer(filters=\"\")\n",
        "english_tokenizer.fit_on_texts(pair[0] for pair in text_pairs)\n",
        "french_tokenizer.fit_on_texts(pair[0] for pair in text_pairs)\n",
        "\n",
        "english_vocab_size = len(english_tokenizer.word_index) + 1\n",
        "french_vocab_size = len(french_tokenizer.word_index) + 1\n",
        "\n",
        "sequence_length = 20\n",
        "batch_size = 64"
      ],
      "metadata": {
        "id": "b60K1vwNAlWJ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It loads the English-French translation dataset from a file (fra.txt).\n",
        "\n",
        "It creates pairs of English and French sentences.\n",
        "\n",
        "It tokenizes both the English and French sentences.\n",
        "\n",
        "It calculates the vocabulary sizes for both languages.\n",
        "\n",
        "It sets the sequence length (maximum number of tokens per sentence) and batch size for training the model."
      ],
      "metadata": {
        "id": "UfIczQjKqZ39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer_decoder(inputs, enc_outputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Attention and Normalization\n",
        "    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x) # Changed le to 1e\n",
        "    res = x + inputs\n",
        "    # Feed Forward Part (Add this part from transformer_encoder)\n",
        "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(res)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    return x + res"
      ],
      "metadata": {
        "id": "Lvy--U8Bk1Yy"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The transformer_decoder function implements the following sequence of operations in a Transformer architecture:\n",
        "\n",
        "Multi-Head Attention: Allows the decoder to focus on different parts of the sequence for each position.\n",
        "\n",
        "Dropout: Applied for regularization.\n",
        "\n",
        "Layer Normalization: Normalizes the output to prevent unstable gradients.\n",
        "\n",
        "Residual Connection: Adds the input back to the output for better gradient flow.\n",
        "\n",
        "Feed-Forward Network: A fully connected layer with ReLU activation to learn higher-level representations.\n",
        "\n",
        "Final Residual Connection: The output of the feed-forward network is added back to the input.\n",
        "\n",
        "This decoder can be used as part of a larger Transformer-based architecture for tasks like machine translation, text generation, etc."
      ],
      "metadata": {
        "id": "AGtdTBBRqtOE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the transformer"
      ],
      "metadata": {
        "id": "QZUhdm2GpFhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the transformer modek architecture\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "  # Attention and Normalization\n",
        "  x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)\n",
        "  x = layers.Dropout(dropout)(x)\n",
        "  x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "  res = x + inputs\n",
        "\n",
        "  # Feed forward part\n",
        "  x = layers.conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(res)\n",
        "  x = layers.Dropout(dropout)(x)\n",
        "  x = layers.conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "  x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "  return x + res"
      ],
      "metadata": {
        "id": "7eLWLhOuCmBX"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The transformer_encoder function implements the following sequence of operations:\n",
        "\n",
        "Multi-Head Attention: The encoder attends to the input sequence using self-attention, allowing it to capture dependencies at different positions in the sequence.\n",
        "\n",
        "Dropout: Regularization to prevent overfitting.\n",
        "\n",
        "Layer Normalization: Normalizes the output of the attention layer to improve stability during training.\n",
        "\n",
        "Residual Connection: Adds the original input back to the output of the attention block, helping with gradient flow.\n",
        "\n",
        "Feed-Forward Network: A fully connected layer (applied position-wise) followed by another convolution, transforming the intermediate representation.\n",
        "\n",
        "Final Layer Normalization: Stabilizes the output of the feed-forward network.\n",
        "\n",
        "Final Residual Connection: Adds the original input back to the output after the feed-forward network, which helps preserve information.\n",
        "\n",
        "This encoder can be stacked multiple times to form a deep Transformer model, which is the core architecture used in models like BERT and GPT."
      ],
      "metadata": {
        "id": "jL0PHrrBrAPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the transformer modek architecture\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "  # Attention and Normalization\n",
        "  x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)\n",
        "  x = layers.Dropout(dropout)(x)\n",
        "  x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "  res = x + inputs\n",
        "\n",
        "  # Feed forward part\n",
        "  x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(res)\n",
        "  x = layers.Dropout(dropout)(x)\n",
        "  x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "  x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "  return x + res\n",
        "\n",
        "\n",
        "def build_model(input_vocab_size, target_size, max_length):\n",
        "  inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"inputs\")\n",
        "  dec_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"dec_inputs\")\n",
        "\n",
        "\n",
        "  # Encoder\n",
        "  enc_padding_mask = keras.layers.Lambda(\n",
        "      lambda x: keras.backend.cast(keras.backend.equal(x,0), keras.backend.floatx()))(inputs)\n",
        "  enc_outputs = keras.layers.Embedding(input_vocab_size, 128)(inputs)\n",
        "  # Use the custom PositionalEncoding class you defined\n",
        "  enc_outputs = PositionalEncoding(max_length, 128)(enc_outputs)\n",
        "  enc_outputs = transformer_encoder(enc_outputs, head_size=128, num_heads=8, ff_dim=512, dropout=0.1) # Changed num_head to num_heads\n",
        "  enc_outputs = layers.Dropout(0.1)(enc_outputs)\n",
        "\n",
        "  # Decoder\n",
        "  look_ahead_mask = keras.layers.Lambda(\n",
        "      lambda x: keras.backend.cast(keras.backend.equal(x,0), keras.backend.floatx()))(dec_inputs)\n",
        "  dec_padding_mask = keras.layers.Lambda(\n",
        "      lambda x: keras.backend.cast(keras.backend.equal(x,0), keras.backend.floatx()))(inputs)\n",
        "\n",
        "  dec_outputs = keras.layers.Embedding(target_vocab_size, 128)(dec_inputs)\n",
        "  # Use the custom PositionalEncoding class you defined\n",
        "  dec_outputs = PositionalEncoding(max_length, 128)(dec_outputs)\n",
        "  dec_outputs = transformer_decoder(dec_outputs, enc_outputs, head_size=128, num_heads=8, ff_dim=512, dropout=0.1) # Changed num_head to num_heads\n",
        "  dec_outputs = layers.Dropout(0.1)(dec_outputs)\n",
        "  outputs = layers.Dense(target_vocab_size, activation=\"softmax\")(dec_outputs)\n",
        "\n",
        "  model = keras.Model(inputs=[inputs, dec_inputs], outputs=outputs)\n",
        "  return model"
      ],
      "metadata": {
        "id": "qjdWGCEyjRoZ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary of the Transformer Architecture\n",
        "Encoder:\n",
        "\n",
        "The encoder takes the input sequence and passes it through a series of self-attention layers (with multi-head attention), followed by a feed-forward network.\n",
        "\n",
        "It uses a positional encoding to add information about the order of tokens in the sequence.\n",
        "\n",
        "Decoder:\n",
        "\n",
        "The decoder takes the target sequence and generates predictions by attending to both the target sequence itself (self-attention) and the encoder's output (cross-attention).\n",
        "\n",
        "Like the encoder, the decoder uses positional encoding to preserve the order of tokens.\n",
        "\n",
        "Final Output:\n",
        "\n",
        "The decoder output is passed through a dense layer with softmax activation to generate a probability distribution over the target vocabulary, from which the next token can be sampled or predicted.\n",
        "\n",
        "This architecture is suitable for sequence-to-sequence tasks, such as machine translation, where the model converts an input sequence (e.g., English) into an output sequence (e.g., French)."
      ],
      "metadata": {
        "id": "um7B2h2yrYrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer_decoder(inputs, enc_outputs, head_size,num_heads, ff_dim, dropout=0):\n",
        "  # Attention and Normalization\n",
        "  x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)\n",
        "  x = layers.Dropout(dropout)(x)\n",
        "  x = layers.LayerNormalization(epsilon=le-6)(x)\n",
        "  res = x + inputs"
      ],
      "metadata": {
        "id": "3x5SijUxEZXN"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(input_vocab_size, target_size, max_length): # The parameter target_size is being passed\n",
        "  inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"inputs\")\n",
        "  dec_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"dec_inputs\")\n",
        "\n",
        "\n",
        "  # Encoder\n",
        "  enc_padding_mask = keras.layers.Lambda(\n",
        "      lambda x: keras.backend.cast(keras.backend.equal(x,0), keras.backend.floatx()))(inputs)\n",
        "  enc_outputs = keras.layers.Embedding(input_vocab_size, 128)(inputs)\n",
        "  # Use the custom PositionalEncoding class you defined\n",
        "  enc_outputs = PositionalEncoding(max_length, 128)(enc_outputs)\n",
        "  enc_outputs = transformer_encoder(enc_outputs, head_size=128, num_heads=8, ff_dim=512, dropout=0.1) # Changed num_head to num_heads\n",
        "  enc_outputs = layers.Dropout(0.1)(enc_outputs)\n",
        "\n",
        "  # Decoder\n",
        "  look_ahead_mask = keras.layers.Lambda(\n",
        "      lambda x: keras.backend.cast(keras.backend.equal(x,0), keras.backend.floatx()))(dec_inputs)\n",
        "  dec_padding_mask = keras.layers.Lambda(\n",
        "      lambda x: keras.backend.cast(keras.backend.equal(x,0), keras.backend.floatx()))(inputs)\n",
        "\n",
        "  dec_outputs = keras.layers.Embedding(target_size, 128)(dec_inputs) # Changed target_vocab_size to target_size\n",
        "  # Use the custom PositionalEncoding class you defined\n",
        "  dec_outputs = PositionalEncoding(max_length, 128)(dec_outputs)\n",
        "  dec_outputs = transformer_decoder(dec_outputs, enc_outputs, head_size=128, num_heads=8, ff_dim=512, dropout=0.1) # Changed num_head to num_heads\n",
        "  dec_outputs = layers.Dropout(0.1)(dec_outputs)\n",
        "  outputs = layers.Dense(target_size, activation=\"softmax\")(dec_outputs) # Changed target_vocab_size to target_size\n",
        "\n",
        "  model = keras.Model(inputs=[inputs, dec_inputs], outputs=outputs)\n",
        "  return model"
      ],
      "metadata": {
        "id": "YrD5wS_pj10Y"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer_decoder(inputs, enc_outputs, head_size,num_heads, ff_dim, dropout=0):\n",
        "  # Attention and Normalization\n",
        "  x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)\n",
        "  x = layers.Dropout(dropout)(x)\n",
        "  x = layers.LayerNormalization(epsilon=1e-6)(x) # Changed le to 1e-6\n",
        "  res = x + inputs\n",
        "  # Feed Forward Part (Add this part from transformer_encoder)\n",
        "  x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(res)\n",
        "  x = layers.Dropout(dropout)(x)\n",
        "  x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "  x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "  return x + res"
      ],
      "metadata": {
        "id": "egwk1bMelCII"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lYG1XQpHpXgO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "id": "v2K7baAbpYD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and train the model\n",
        "transformer_model = build_model(english_vocab_size, french_vocab_size, sequence_length)\n",
        "transformer_model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "english_sequence = english_tokenizer.texts_to_sequences(pair[0] for pair in text_pairs)\n",
        "french_sequence = french_tokenizer.texts_to_sequences(pair[1] for pair in text_pairs)\n",
        "\n",
        "english_sequence = keras.preprocessing.sequence.pad_sequences(english_sequence, maxlen=sequence_length, padding=\"post\")\n",
        "french_sequence = keras.preprocessing.sequence.pad_sequences(french_sequence, maxlen=sequence_length, padding=\"post\")\n",
        "\n",
        "# Remove the sample_weight parameter and keep only batch_size\n",
        "transformer_model.fit([english_sequence, french_sequence[:, :-1]],\n",
        "                    french_sequence[:, 1:],\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhfT1TFflNrn",
        "outputId": "14a4255a-7230-4c9a-dd79-7f55f7ddd012"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m2612/2612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 29ms/step - accuracy: 0.9900 - loss: 0.2315\n",
            "Epoch 2/10\n",
            "\u001b[1m2612/2612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 27ms/step - accuracy: 0.9982 - loss: 0.0104\n",
            "Epoch 3/10\n",
            "\u001b[1m2612/2612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 27ms/step - accuracy: 0.9987 - loss: 0.0069\n",
            "Epoch 4/10\n",
            "\u001b[1m2612/2612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 27ms/step - accuracy: 0.9990 - loss: 0.0050\n",
            "Epoch 5/10\n",
            "\u001b[1m2612/2612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 27ms/step - accuracy: 0.9992 - loss: 0.0039\n",
            "Epoch 6/10\n",
            "\u001b[1m2612/2612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 27ms/step - accuracy: 0.9993 - loss: 0.0032\n",
            "Epoch 7/10\n",
            "\u001b[1m2612/2612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 27ms/step - accuracy: 0.9995 - loss: 0.0024\n",
            "Epoch 8/10\n",
            "\u001b[1m2612/2612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 27ms/step - accuracy: 0.9995 - loss: 0.0021\n",
            "Epoch 9/10\n",
            "\u001b[1m2612/2612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 27ms/step - accuracy: 0.9996 - loss: 0.0019\n",
            "Epoch 10/10\n",
            "\u001b[1m2612/2612\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 27ms/step - accuracy: 0.9997 - loss: 0.0015\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f6e00c0b490>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preparation:\n",
        "\n",
        "Convert English and French sentences into sequences of integers.\n",
        "\n",
        "Pad the sequences to ensure consistent length.\n",
        "\n",
        "Model Building:\n",
        "\n",
        "Build the Transformer model with encoder and decoder components.\n",
        "\n",
        "Model Compilation:\n",
        "\n",
        "Compile the model using Adam optimizer and sparse categorical cross-entropy loss.\n",
        "\n",
        "Model Training:\n",
        "\n",
        "Train the model using the input English sequences and target French sequences for 10 epochs."
      ],
      "metadata": {
        "id": "VUNAvswlsTVx"
      }
    }
  ]
}